{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHPBJED1TNmvMlHESlYJE+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arbin34/heruk/blob/main/LM_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TRQx-8n_3S0",
        "outputId": "6bd4c4d6-ffc9-4c1b-986f-9ad6e656ffbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wxconv in /usr/local/lib/python3.10/dist-packages (1.0.0.0)\n",
            "Requirement already satisfied: pbr<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wxconv) (2.1.0)\n",
            "Requirement already satisfied: six<2.0,>=1.12 in /usr/local/lib/python3.10/dist-packages (from wxconv) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wxconv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import CONSTANTS\n",
        "import logging\n",
        "from wxconv import WXC\n",
        "\n",
        "# Define connectives as dictionaries for better organization\n",
        "SIMPLE_CONNECTIVES = { 'कि', 'और', 'एवं', 'इसलिए', 'क्योंकि', 'जबकि', 'तथा', 'ताकि', 'मगर', 'लेकिन', 'किंतु', 'परंतु', 'फिर', 'तथापि',\n",
        "                      'नहीं तो'}\n",
        "COMPLEX_CONNECTIVES = {}\n",
        "\n",
        "def log(message, log_type='OK'):\n",
        "    \"\"\"Generates log message in a predefined format.\"\"\"\n",
        "    print(f'[{log_type}] : {message}')\n",
        "    if log_type == 'ERROR':\n",
        "        sys.exit()\n",
        "def read_input(file_path):\n",
        "    \"\"\"Reads and returns a dictionary with sentence_id as the key and the sentence as the value.\"\"\"\n",
        "    log(f'File ~ {file_path}')\n",
        "    input_data = {}\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            lines = file.readlines()\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    sentence_info = line.split(' ', 1)\n",
        "                    key, value = sentence_info[0], sentence_info[1].strip()\n",
        "                    input_data[key] = value\n",
        "            log('File data read.')\n",
        "    except FileNotFoundError:\n",
        "        log('No such file found.', 'ERROR')\n",
        "        sys.exit()\n",
        "    return input_data\n",
        "\n",
        "def clean(word):\n",
        "    \"\"\"Clean concept words by removing numbers and special characters using regex.\"\"\"\n",
        "    return re.sub(r'[0-9!@#$%^&*()]', '', word)\n",
        "\n",
        "def validate_sentence(sentence):\n",
        "    \"\"\"Validate if the sentence is not empty and contains non-digit characters.\"\"\"\n",
        "    return bool(sentence) and any(char.isalpha() for char in sentence)\n",
        "\n",
        "def split_sentence(sentence):\n",
        "    \"\"\"Split the sentence based on both SIMPLE_CONNECTIVES and COMPLEX_CONNECTIVES.\"\"\"\n",
        "    connectives = '|'.join(map(re.escape, SIMPLE_CONNECTIVES | set(COMPLEX_CONNECTIVES.keys())))\n",
        "    parts = re.split(f'({connectives})', sentence)\n",
        "    return [part.strip() for part in parts if part.strip()]\n",
        "\n",
        "def sanitize_input(sentence):\n",
        "    wx_format = WXC(order=\"utf2wx\", lang=\"hin\")\n",
        "    generate_wx_text = wx_format.convert(sentence)\n",
        "    clean_wx_text = \" \".join([clean(word) for word in generate_wx_text.strip().split()])\n",
        "    hindi_format = WXC(order=\"wx2utf\", lang=\"hin\")\n",
        "    clean_hindi_text = hindi_format.convert(clean_wx_text).strip()\n",
        "    if clean_hindi_text.endswith('.'):\n",
        "        clean_hindi_text = clean_hindi_text[:-1] + \" ।\"\n",
        "    return clean_hindi_text\n",
        "\n",
        "def write_output(dictionary, file_path, manual_evaluation):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        for key, values in dictionary.items():\n",
        "            # Initialize a counter for sub-sentences\n",
        "            sub_sentence_counter = 1\n",
        "            for item in values:\n",
        "                tag = 'Manual evaluation' if item in manual_evaluation else 'None'\n",
        "                # Each sub-sentence should end with poornaviram\n",
        "                if not item.endswith('।'):\n",
        "                    item += ' ।'\n",
        "                # Construct the full sentence line with a sub-sentence identifier\n",
        "                line = f'{key}{string.ascii_lowercase[sub_sentence_counter - 1]}  {item}  {tag}\\n'\n",
        "                file.write(line)\n",
        "                sub_sentence_counter += 1\n",
        "    log(\"Output file written successfully\")\n",
        "\n",
        "def separate_sentences_with_connectives(sentence):\n",
        "    simpler_sentences = []\n",
        "    connective_indices = []\n",
        "\n",
        "    for connective in SIMPLE_CONNECTIVES | set(COMPLEX_CONNECTIVES.keys()):\n",
        "        if connective in sentence:\n",
        "            indices = [m.start() for m in re.finditer(connective, sentence)]\n",
        "            connective_indices.extend((i, i + len(connective)) for i in indices)\n",
        "\n",
        "    connective_indices.sort()\n",
        "    start = 0\n",
        "    for i, j in connective_indices:\n",
        "        part = sentence[start:i].strip()\n",
        "        if part:\n",
        "            simpler_sentences.append(part)\n",
        "        start = j\n",
        "\n",
        "    last_part = sentence[start:].strip()\n",
        "    if last_part:\n",
        "        simpler_sentences.append(last_part)\n",
        "\n",
        "    return simpler_sentences\n",
        "\n",
        "# Other functions and main code remain mostly the same as the previous response.\n",
        "\n",
        "\n",
        "def is_prev_word_verb(parser_output, index):\n",
        "    try:\n",
        "        with open(parser_output, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "            for i in range(len(lines)):\n",
        "                if i == index:\n",
        "                    lineContent = lines[i].strip().split()\n",
        "                    if len(lineContent) > 0 and (lineContent[1] == 'VM' or lineContent[1] == 'VAUX'):\n",
        "                        return True\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        log('No such File found.', 'ERROR')\n",
        "        sys.exit()\n",
        "    return False\n",
        "\n",
        "def get_index_of_word(words, value):\n",
        "    index = -1\n",
        "    for i in range(len(words)):\n",
        "        if words[i] == value:\n",
        "            index = i\n",
        "            break\n",
        "    return index\n",
        "\n",
        "def get_word_at_index(words, index):\n",
        "    word = \"\"\n",
        "    for i in range(len(words)):\n",
        "        if i == index:\n",
        "            word = words[i]\n",
        "            break\n",
        "    return word\n",
        "\n",
        "def get_POS_by_index(parser_output, index):\n",
        "    tag = ''\n",
        "    try:\n",
        "        with open(parser_output, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "            for i in range(len(lines)):\n",
        "                row = lines[i].strip().split()\n",
        "                if len(row) == 10 and row[0] == str(index + 1):\n",
        "                    tag = row[3]\n",
        "                    break\n",
        "            return tag\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        log('No such File found.', 'ERROR')\n",
        "        sys.exit()\n",
        "\n",
        "def get_dep_by_index(parser_output, index):\n",
        "    dep = ''\n",
        "    try:\n",
        "        with open(parser_output, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "            for i in range(len(lines)):\n",
        "                row = lines[i].strip().split()\n",
        "                if len(row) == 10 and row[0] == str(index+1):\n",
        "                    dep = row[7]\n",
        "                    break\n",
        "            return dep\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        log('No such File found.', 'ERROR')\n",
        "        sys.exit()\n",
        "\n",
        "def breakPairConnective(sentence, manual_evaluation):\n",
        "    # This function return list of sentences if a paired connective is found else returns an empty list\n",
        "    simpler_sentences = []\n",
        "    BREAK_SENTENCE = False\n",
        "    # Tokenize the sentence by splitting it into words\n",
        "    tokens = sentence.split()\n",
        "    # Iterate through the tokens to find connectives and split the sentence\n",
        "    for i in range(len(tokens)):\n",
        "        token = tokens[i]\n",
        "        # Check if the token is a paired-connective\n",
        "        if token in CONSTANTS.COMPLEX_CONNECTIVES:\n",
        "            pair_value_lst = CONSTANTS.COMPLEX_CONNECTIVES[token]\n",
        "            for pair_value in pair_value_lst:\n",
        "                if pair_value in sentence:\n",
        "                    pair_value = pair_value.strip().split()[0]\n",
        "                    index_of_pair_value = get_index_of_word(tokens, pair_value)\n",
        "                    if not (index_of_pair_value == -1):\n",
        "                        get_tagger_output(sentence)\n",
        "                        if is_prev_word_verb(CONSTANTS.PARSER_OUTPUT, index_of_pair_value - 1):\n",
        "                            tokens.pop(i)\n",
        "                            index_of_pair_value = index_of_pair_value - 1\n",
        "                            sent1 = tokens[:index_of_pair_value]\n",
        "                            sent2 = tokens[index_of_pair_value:]\n",
        "                            simpler_sentences.append(\" \".join(sent1))\n",
        "                            simpler_sentences.append(\" \".join(sent2))\n",
        "                            BREAK_SENTENCE = True\n",
        "                            break\n",
        "                        else:\n",
        "                            manual_evaluation.append(sentence)\n",
        "            if BREAK_SENTENCE:\n",
        "                break\n",
        "    return simpler_sentences\n",
        "\n",
        "def breakSimpleConnective(sentence, manual_evaluation):\n",
        "    # This function returns a list of sentences if a simple connective is found; otherwise, it returns an empty list\n",
        "    simpler_sentences = []\n",
        "    # Tokenize the sentence by splitting it into words\n",
        "    tokens = sentence.split()\n",
        "    for i in range(len(tokens)):  # Use range(len(tokens)) to iterate through indices\n",
        "        token = tokens[i]\n",
        "        # 'नहीं तो' is a simple connective\n",
        "        if token == 'नहीं':\n",
        "            following_word = get_word_at_index(tokens, i + 1)\n",
        "            if following_word == 'तो':\n",
        "                token = 'नहीं तो'\n",
        "\n",
        "        # Check if the token is a connective\n",
        "        if token in CONSTANTS.SIMPLE_CONNECTIVES:\n",
        "            if token == 'और' or token == 'एवं' or token == 'तथा' or token == 'या':\n",
        "                get_parser_output(sentence)\n",
        "                token_POS = get_POS_by_index(CONSTANTS.PARSER_OUTPUT, i)\n",
        "                token_dep = get_dep_by_index(CONSTANTS.PARSER_OUTPUT, i)\n",
        "                get_tagger_output(sentence)\n",
        "                if token_POS == 'CC' and token_dep == 'main' and is_prev_word_verb(CONSTANTS.PARSER_OUTPUT, i - 1):\n",
        "                    sent1 = tokens[:i]\n",
        "                    sent2 = tokens[i:]\n",
        "                    simpler_sentences.append(\" \".join(sent1))\n",
        "                    simpler_sentences.append(\" \".join(sent2))\n",
        "                    break\n",
        "                elif i > 1:\n",
        "                    manual_evaluation.append(sentence)\n",
        "\n",
        "            else:\n",
        "                get_tagger_output(sentence)\n",
        "                if is_prev_word_verb(CONSTANTS.PARSER_OUTPUT, i - 1):\n",
        "                    sent1 = tokens[:i]\n",
        "                    sent2 = tokens[i:]\n",
        "                    simpler_sentences.append(\" \".join(sent1))\n",
        "                    simpler_sentences.append(\" \".join(sent2))\n",
        "                    break\n",
        "                elif i > 1:\n",
        "                    manual_evaluation.append(sentence)\n",
        "\n",
        "    return simpler_sentences\n",
        "\n",
        "\n",
        "def write_input_in_parser_input(file_path, sentence):\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.truncate()\n",
        "        file.write(sentence)\n",
        "        file.close()\n",
        "\n",
        "def get_tagger_output(sentence):\n",
        "    parser_input_file = CONSTANTS.PARSER_INPUT\n",
        "    write_input_in_parser_input(parser_input_file, sentence)\n",
        "    with open(CONSTANTS.PARSER_OUTPUT, 'w') as file:\n",
        "        file.truncate()\n",
        "    os.system(\"isc-tagger -i p_parser_input.txt -o p_parser_output.txt\")\n",
        "\n",
        "def get_parser_output(sentence):\n",
        "    parser_input_file = CONSTANTS.PARSER_INPUT\n",
        "    write_input_in_parser_input(parser_input_file, sentence)\n",
        "    with open(CONSTANTS.PARSER_OUTPUT, 'w') as file:\n",
        "        file.truncate()\n",
        "    os.system(\"isc-parser -i p_parser_input.txt -o p_parser_output.txt\")\n",
        "\n",
        "def breakAllPairedConnective(sentence, allPairedConnectiveList, manual_evaluation):\n",
        "    simpler_sentences = breakPairConnective(sentence, manual_evaluation)\n",
        "    if len(simpler_sentences) == 0:\n",
        "        allPairedConnectiveList.append(sentence)\n",
        "        return\n",
        "\n",
        "    for s in simpler_sentences:\n",
        "        breakAllPairedConnective(s, allPairedConnectiveList, manual_evaluation)\n",
        "\n",
        "    return\n",
        "\n",
        "def breakAllSimpleConnective(sentence, allSimpleConnectiveList, manual_evaluation):\n",
        "    simpler_sentences = breakSimpleConnective(sentence, manual_evaluation)\n",
        "    if len(simpler_sentences) == 0:\n",
        "        allSimpleConnectiveList.append(sentence)\n",
        "        return\n",
        "\n",
        "    for s in simpler_sentences:\n",
        "        breakAllSimpleConnective(s, allSimpleConnectiveList, manual_evaluation)\n",
        "\n",
        "    return\n",
        "if __name__ == '__main__':\n",
        "    input_data = read_input(CONSTANTS.INPUT_FILE)\n",
        "    output_data = {}\n",
        "    manual_evaluation = []\n",
        "\n",
        "    for key, value in input_data.items():\n",
        "        if validate_sentence(value):\n",
        "            value = sanitize_input(value)\n",
        "            simpler_sentences = separate_sentences_with_connectives(value)\n",
        "\n",
        "            # First break the sentence by pair connectives\n",
        "            allPairedConnectiveList = []\n",
        "            for s in simpler_sentences:\n",
        "                breakAllPairedConnective(s, allPairedConnectiveList, manual_evaluation)\n",
        "            allSimpleConnectiveList = []\n",
        "\n",
        "            for s in allPairedConnectiveList:\n",
        "                breakAllSimpleConnective(s, allSimpleConnectiveList, manual_evaluation)\n",
        "        else:\n",
        "            allSimpleConnectiveList = ['Invalid input']\n",
        "\n",
        "        output_data[key] = allSimpleConnectiveList\n",
        "\n",
        "    write_output(output_data, CONSTANTS.OUTPUT_FILE, manual_evaluation)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNGAZ0_mXU6K",
        "outputId": "6c5ce2d0-32fc-4d95-be2d-c8d31adbead1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] : File ~ input.txt\n",
            "[OK] : File data read.\n",
            "[OK] : Output file written successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import CONSTANTS\n",
        "import logging\n",
        "from wxconv import WXC\n",
        "\n",
        "# Define connectives as dictionaries for better organization\n",
        "SIMPLE_CONNECTIVES = {'और', 'एवं' ,'इसलिए', 'क्योंकि', 'जबकि' ,'तथा', 'ताकि', 'मगर', 'लेकिन', 'किंतु', 'परंतु', 'फिर',\n",
        "                      'या', 'तथापि','नहीं तो', 'चूंकि', 'चूँकि', 'वरना','अन्यथा', 'बशर्तें', 'हालाँकि', 'इसीलिये', 'इसीलिए' ,\n",
        "                      'इसलिए', 'अथवा', 'अतः', 'अर्थात्', 'जब', 'तो'}\n",
        "COMPLEX_CONNECTIVES = {\n",
        "    'क्योंकि': ['इसलिए', 'इसके कारण', 'इसलिए', 'क्योंकि'],\n",
        "    # Add more complex connectives and their alternatives here\n",
        "}\n",
        "\n",
        "def log(message, log_type='OK'):\n",
        "    \"\"\"Generates log message in a predefined format.\"\"\"\n",
        "    print(f'[{log_type}] : {message}')\n",
        "    if log_type == 'ERROR':\n",
        "        sys.exit()\n",
        "\n",
        "def read_input(file_path):\n",
        "    \"\"\"Reads and returns a dictionary with sentence_id as the key and the sentence as the value.\"\"\"\n",
        "    log(f'File ~ {file_path}')\n",
        "    input_data = {}\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            lines = file.readlines()\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    sentence_info = line.split(' ', 1)\n",
        "                    key, value = sentence_info[0], sentence_info[1].strip()\n",
        "                    input_data[key] = value\n",
        "            log('File data read.')\n",
        "    except FileNotFoundError:\n",
        "        log('No such file found.', 'ERROR')\n",
        "        sys.exit()\n",
        "    return input_data\n",
        "\n",
        "def clean(word):\n",
        "    \"\"\"Clean concept words by removing numbers and special characters using regex.\"\"\"\n",
        "    return re.sub(r'[0-9!@#$%^&*()]', '', word)\n",
        "\n",
        "def validate_sentence(sentence):\n",
        "    \"\"\"Validate if the sentence is not empty and contains non-digit characters.\"\"\"\n",
        "    return bool(sentence) and any(char.isalpha() for char in sentence)\n",
        "\n",
        "def split_sentence(sentence):\n",
        "    \"\"\"Split the sentence based on both SIMPLE_CONNECTIVES and COMPLEX_CONNECTIVES.\"\"\"\n",
        "    connectives = '|'.join(map(re.escape, SIMPLE_CONNECTIVES | set(COMPLEX_CONNECTIVES.keys())))\n",
        "    parts = re.split(f'({connectives})', sentence)\n",
        "    return [part.strip() for part in parts if part.strip()]\n",
        "\n",
        "def sanitize_input(sentence):\n",
        "    wx_format = WXC(order=\"utf2wx\", lang=\"hin\")\n",
        "    generate_wx_text = wx_format.convert(sentence)\n",
        "    clean_wx_text = \" \".join([clean(word) for word in generate_wx_text.strip().split()])\n",
        "    hindi_format = WXC(order=\"wx2utf\", lang=\"hin\")\n",
        "    clean_hindi_text = hindi_format.convert(clean_wx_text).strip()\n",
        "    if clean_hindi_text.endswith('.'):\n",
        "        clean_hindi_text = clean_hindi_text[:-1] + \" ।\"\n",
        "    return clean_hindi_text\n",
        "\n",
        "def write_output(dictionary, file_path, manual_evaluation):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        for key, values in dictionary.items():\n",
        "            # Initialize a counter for sub-sentences\n",
        "            sub_sentence_counter = 1\n",
        "            for item in values:\n",
        "                tag = 'Manual evaluation' if item in manual_evaluation else 'None'\n",
        "                # Each sub-sentence should end with poornaviram\n",
        "                if not item.endswith('।'):\n",
        "                    item += ' ।'\n",
        "                # Construct the full sentence line with a sub-sentence identifier\n",
        "                line = f'{key}{string.ascii_lowercase[sub_sentence_counter - 1]}  {item}  {tag}\\n'\n",
        "                file.write(line)\n",
        "                sub_sentence_counter += 1\n",
        "    log(\"Output file written successfully\")\n",
        "\n",
        "def separate_sentences_with_connectives(sentence):\n",
        "    simpler_sentences = []\n",
        "    connective_indices = []\n",
        "\n",
        "    for connective in SIMPLE_CONNECTIVES | set(COMPLEX_CONNECTIVES.keys()):\n",
        "        if connective in sentence:\n",
        "            indices = [m.start() for m in re.finditer(connective, sentence)]\n",
        "            connective_indices.extend((i, i + len(connective)) for i in indices)\n",
        "\n",
        "    connective_indices.sort()\n",
        "    start = 0\n",
        "    for i, j in connective_indices:\n",
        "        part = sentence[start:i].strip()\n",
        "        if part:\n",
        "            simpler_sentences.append(part)\n",
        "        start = j\n",
        "\n",
        "    last_part = sentence[start:].strip()\n",
        "    if last_part:\n",
        "        simpler_sentences.append(last_part)\n",
        "\n",
        "    return simpler_sentences\n",
        "\n",
        "# Other functions and main code remain mostly the same as the previous response.\n",
        "\n",
        "\n",
        "def is_prev_word_verb(parser_output, index):\n",
        "    try:\n",
        "        with open(parser_output, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "            for i in range(len(lines)):\n",
        "                if i == index:\n",
        "                    lineContent = lines[i].strip().split()\n",
        "                    if len(lineContent) > 0 and (lineContent[1] == 'VM' or lineContent[1] == 'VAUX'):\n",
        "                        return True\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        log('No such File found.', 'ERROR')\n",
        "        sys.exit()\n",
        "    return False\n",
        "\n",
        "def get_index_of_word(words, value):\n",
        "    index = -1\n",
        "    for i in range(len(words)):\n",
        "        if words[i] == value:\n",
        "            index = i\n",
        "            break\n",
        "    return index\n",
        "\n",
        "def get_word_at_index(words, index):\n",
        "    word = \"\"\n",
        "    for i in range(len(words)):\n",
        "        if i == index:\n",
        "            word = words[i]\n",
        "            break\n",
        "    return word\n",
        "\n",
        "def get_POS_by_index(parser_output, index):\n",
        "    tag = ''\n",
        "    try:\n",
        "        with open(parser_output, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "            for i in range(len(lines)):\n",
        "                row = lines[i].strip().split()\n",
        "                if len(row) == 10 and row[0] == str(index + 1):\n",
        "                    tag = row[3]\n",
        "                    break\n",
        "            return tag\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        log('No such File found.', 'ERROR')\n",
        "        sys.exit()\n",
        "\n",
        "def get_dep_by_index(parser_output, index):\n",
        "    dep = ''\n",
        "    try:\n",
        "        with open(parser_output, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "            for i in range(len(lines)):\n",
        "                row = lines[i].strip().split()\n",
        "                if len(row) == 10 and row[0] == str(index+1):\n",
        "                    dep = row[7]\n",
        "                    break\n",
        "            return dep\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        log('No such File found.', 'ERROR')\n",
        "        sys.exit()\n",
        "\n",
        "def breakPairConnective(sentence, manual_evaluation):\n",
        "    # This function return list of sentences if a paired connective is found else returns an empty list\n",
        "    simpler_sentences = []\n",
        "    BREAK_SENTENCE = False\n",
        "    # Tokenize the sentence by splitting it into words\n",
        "    tokens = sentence.split()\n",
        "    # Iterate through the tokens to find connectives and split the sentence\n",
        "    for i in range(len(tokens)):\n",
        "        token = tokens[i]\n",
        "        # Check if the token is a paired-connective\n",
        "        if token in CONSTANTS.COMPLEX_CONNECTIVES:\n",
        "            pair_value_lst = CONSTANTS.COMPLEX_CONNECTIVES[token]\n",
        "            for pair_value in pair_value_lst:\n",
        "                if pair_value in sentence:\n",
        "                    pair_value = pair_value.strip().split()[0]\n",
        "                    index_of_pair_value = get_index_of_word(tokens, pair_value)\n",
        "                    if not (index_of_pair_value == -1):\n",
        "                        get_tagger_output(sentence)\n",
        "                        if is_prev_word_verb(CONSTANTS.PARSER_OUTPUT, index_of_pair_value - 1):\n",
        "                            tokens.pop(i)\n",
        "                            index_of_pair_value = index_of_pair_value - 1\n",
        "                            sent1 = tokens[:index_of_pair_value]\n",
        "                            sent2 = tokens[index_of_pair_value:]\n",
        "                            simpler_sentences.append(\" \".join(sent1))\n",
        "                            simpler_sentences.append(\" \".join(sent2))\n",
        "                            BREAK_SENTENCE = True\n",
        "                            break\n",
        "                        else:\n",
        "                            manual_evaluation.append(sentence)\n",
        "            if BREAK_SENTENCE:\n",
        "                break\n",
        "    return simpler_sentences\n",
        "\n",
        "def breakSimpleConnective(sentence, manual_evaluation):\n",
        "    # This function returns a list of sentences if a simple connective is found; otherwise, it returns an empty list\n",
        "    simpler_sentences = []\n",
        "    # Tokenize the sentence by splitting it into words\n",
        "    tokens = sentence.split()\n",
        "    for i in range(len(tokens)):  # Use range(len(tokens)) to iterate through indices\n",
        "        token = tokens[i]\n",
        "        # 'नहीं तो' is a simple connective\n",
        "        if token == 'नहीं':\n",
        "            following_word = get_word_at_index(tokens, i + 1)\n",
        "            if following_word == 'तो':\n",
        "                token = 'नहीं तो'\n",
        "\n",
        "        # Check if the token is a connective\n",
        "        if token in CONSTANTS.SIMPLE_CONNECTIVES:\n",
        "            if token == 'और' or token == 'एवं' or token == 'तथा' or token == 'या':\n",
        "                get_parser_output(sentence)\n",
        "                token_POS = get_POS_by_index(CONSTANTS.PARSER_OUTPUT, i)\n",
        "                token_dep = get_dep_by_index(CONSTANTS.PARSER_OUTPUT, i)\n",
        "                get_tagger_output(sentence)\n",
        "                if token_POS == 'CC' and token_dep == 'main' and is_prev_word_verb(CONSTANTS.PARSER_OUTPUT, i - 1):\n",
        "                    sent1 = tokens[:i]\n",
        "                    sent2 = tokens[i:]\n",
        "                    simpler_sentences.append(\" \".join(sent1))\n",
        "                    simpler_sentences.append(\" \".join(sent2))\n",
        "                    break\n",
        "                elif i > 1:\n",
        "                    manual_evaluation.append(sentence)\n",
        "\n",
        "            else:\n",
        "                get_tagger_output(sentence)\n",
        "                if is_prev_word_verb(CONSTANTS.PARSER_OUTPUT, i - 1):\n",
        "                    sent1 = tokens[:i]\n",
        "                    sent2 = tokens[i:]\n",
        "                    simpler_sentences.append(\" \".join(sent1))\n",
        "                    simpler_sentences.append(\" \".join(sent2))\n",
        "                    break\n",
        "                elif i > 1:\n",
        "                    manual_evaluation.append(sentence)\n",
        "\n",
        "    return simpler_sentences\n",
        "\n",
        "\n",
        "def write_input_in_parser_input(file_path, sentence):\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.truncate()\n",
        "        file.write(sentence)\n",
        "        file.close()\n",
        "\n",
        "def get_tagger_output(sentence):\n",
        "    parser_input_file = CONSTANTS.PARSER_INPUT\n",
        "    write_input_in_parser_input(parser_input_file, sentence)\n",
        "    with open(CONSTANTS.PARSER_OUTPUT, 'w') as file:\n",
        "        file.truncate()\n",
        "    os.system(\"isc-tagger -i p_parser_input.txt -o p_parser_output.txt\")\n",
        "\n",
        "def get_parser_output(sentence):\n",
        "    parser_input_file = CONSTANTS.PARSER_INPUT\n",
        "    write_input_in_parser_input(parser_input_file, sentence)\n",
        "    with open(CONSTANTS.PARSER_OUTPUT, 'w') as file:\n",
        "        file.truncate()\n",
        "    os.system(\"isc-parser -i p_parser_input.txt -o p_parser_output.txt\")\n",
        "\n",
        "def breakAllPairedConnective(sentence, allPairedConnectiveList, manual_evaluation):\n",
        "    simpler_sentences = breakPairConnective(sentence, manual_evaluation)\n",
        "    if len(simpler_sentences) == 0:\n",
        "        allPairedConnectiveList.append(sentence)\n",
        "        return\n",
        "\n",
        "    for s in simpler_sentences:\n",
        "        breakAllPairedConnective(s, allPairedConnectiveList, manual_evaluation)\n",
        "\n",
        "    return\n",
        "\n",
        "def breakAllSimpleConnective(sentence, allSimpleConnectiveList, manual_evaluation):\n",
        "    simpler_sentences = breakSimpleConnective(sentence, manual_evaluation)\n",
        "    if len(simpler_sentences) == 0:\n",
        "        allSimpleConnectiveList.append(sentence)\n",
        "        return\n",
        "\n",
        "    for s in simpler_sentences:\n",
        "        breakAllSimpleConnective(s, allSimpleConnectiveList, manual_evaluation)\n",
        "\n",
        "    return\n",
        "if __name__ == '__main__':\n",
        "    input_data = read_input(CONSTANTS.INPUT_FILE)\n",
        "    output_data = {}\n",
        "    manual_evaluation = []\n",
        "\n",
        "    for key, value in input_data.items():\n",
        "        if validate_sentence(value):\n",
        "            value = sanitize_input(value)\n",
        "            simpler_sentences = separate_sentences_with_connectives(value)\n",
        "\n",
        "            # First break the sentence by pair connectives\n",
        "            allPairedConnectiveList = []\n",
        "            for s in simpler_sentences:\n",
        "                breakAllPairedConnective(s, allPairedConnectiveList, manual_evaluation)\n",
        "            allSimpleConnectiveList = []\n",
        "\n",
        "            for s in allPairedConnectiveList:\n",
        "                breakAllSimpleConnective(s, allSimpleConnectiveList, manual_evaluation)\n",
        "        else:\n",
        "            allSimpleConnectiveList = ['Invalid input']\n",
        "\n",
        "        output_data[key] = allSimpleConnectiveList\n",
        "\n",
        "    write_output(output_data, CONSTANTS.OUTPUT_FILE, manual_evaluation)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGA-5KnaYurX",
        "outputId": "9e3dd9a2-b8ac-45a7-83eb-eb303b33d2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] : File ~ input.txt\n",
            "[OK] : File data read.\n",
            "[OK] : Output file written successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6GwIHiycZBkW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}